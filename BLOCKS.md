## Входы

### train_df (Spark)
- **Назначение**: Основной набор данных для обучения моделей и расчёта статистик. Используется для всех шагов предобработки, обучения кодировщиков/скейлеров, вычисления метрик и принятия решений об исключении/включении признаков.
- **Что на входе**: Spark DataFrame, содержащий как минимум колонки `index_col` и `target_col` (их имена задаются в `common.index_col` и `common.target_col` в `my_config.yaml`). Допустимы числовые и строковые типы, `DecimalType` будет приведён к `FloatType`. Колонки из `common.skip_cols` игнорируются в трансформациях.
- **Результат**: Передаётся внутрь `PreprocessingPipeline` как `train_sdf`, проходит все выбранные шаги пайплайна. В конце сохраняется в выбранном формате под именем `train{suffix}` по пути `result.save_path` (для Parquet) либо `result.save_path.train{suffix}` (для Hive).

### valid_df (Spark)
- **Назначение**: Используется как валидационная выборка для совместной обработки с train при некоторых шагах (например, дедупликация, adversarial-отбор) и для последующего сохранения обработанной валидной выборки.
- **Что на входе**: Опциональный Spark DataFrame с тем же набором колонок и типами, что и у `train_df`. Наличие `index_col` обязательно; наличие `target_col` желательно для валидной оценки, но не требуется для всех шагов.
- **Результат**: Передаётся как `valid_sdf` (в коде — `test_sdf`), проходит те же трансформации, что и train. Сохраняется как `test{suffix}` в Parquet (`result.save_path`) или как `result.save_path.test{suffix}` в Hive.

### scoring_df (Spark) опц.
- **Назначение**: Набор данных для скоринга/инференса, на который применяются все преобразования, обученные по train. Не участвует в шаге adversarial-отбора, чтобы избегать «подглядывания» и смещения.
- **Что на входе**: Опциональный Spark DataFrame с совместимой схемой c train/valid. Колонка `target_col` не требуется.
- **Результат**: Передаётся как `scoring_sdf`, преобразуется с использованием сохранённых трансформеров/параметров. Сохраняется как `scoring{suffix}` в формате и по пути, задаваемым `result.format_scoring` и `result.save_path_scoring`.

## Инициализация

### Hive context, Spark context
- **Назначение**: Контексты выполнения Spark (RDS/кластер) и SQL-операций: чтение/запись таблиц Hive, выполнение `saveAsTable`, SQL-запросов и распределённых вычислений.
- **Что на входе**: Объекты `sc` (SparkContext) и `sqlContext` (SparkSession/SQLContext), например, созданные через вспомогательный хелпер (`SparkHelper`).
- **Результат**: Доступ к экосистеме Spark/Hive для всего пайплайна, включая сохранение в таблицы при выборе формата `hive`.

### config.yaml
- **Назначение**: Единый источник конфигурации пайплайна: пути артефактов, список шагов `pipeline_steps`, правила заполнения пропусков, параметры ребалансировки, кодирования категориальных признаков, масштабирования числовых, настройки селекторов фичей и форматов сохранения.
- **Что на входе**: YAML-файл (см. `my_config.yaml`) c секциями: `artifacts`, `sampling`, `common` (`index_col`, `target_col`, `skip_cols`, `force_drop_cols`, `n_jobs`, `task_type`), `preprocessing` (подсекции шагов), `pipeline_steps`, `result`.
- **Результат**: Загружается при создании `PreprocessingPipeline`, параметры становятся доступными всем шагам и вспомогательным классам (селекторы, энкодеры, скейлеры).

### PreprocessingPipeline()
- **Назначение**: Оркестратор предобработки. Отвечает за исполнение шагов по порядку, ведение и обновление списков признаков, взаимодействие со Spark, хранение и сохранение артефактов и результатов.
- **Что на входе**: Путь к конфигу, `sc`, `sqlContext`.
- **Результат**: Экземпляр с инициализированными полями: пути артефактов, рабочие списки `cat_cols`, `num_cols`, `final_features_list`, держатели `encoder`/`scaler`, карта `preprocessing_methods` для динамического вызова шагов.

## Запуск

### .run_preprocessing()
- **Назначение**: Полный запуск пайплайна: приём датафреймов, (опциональное) сэмплирование, инициализация списков колонок, последовательный вызов шагов из `pipeline_steps`, финальное сохранение артефактов и выборок.
- **Что на входе**: `train_sdf` (обязательно), `valid_sdf` (опционально), `scoring_sdf` (опционально). Конфигурация из `config.yaml`.
- **Результат**: Кортеж `(train_sdf, valid_sdf, scoring_sdf)` после всех трансформаций; сохранённые артефакты и датасеты в соответствии с секцией `result` конфигурации.

## Базовые шаги

### sampling – сэмплирование данных
- **Назначение**: Ускорение обучения и отбора признаков за счёт случайной подвыборки строк train/valid при больших объёмах данных.
- **Что на входе**: Параметры `sampling.use_sampling`, `sampling.train_sample_size`, `sampling.test_sample_size`. Входные датафреймы `train_sdf` и (опционально) `valid_sdf`.
- **Результат**: Обновлённые `train_sdf`/`valid_sdf` с уменьшенным числом строк; вывод размеров после сэмплирования для контроля.

### определение типов фичей
- **Назначение**: Автоматическая инициализация списков признаков: категориальные (`cat_cols`) и числовые (`num_cols`), а также агрегированный `final_features_list` для всех последующих шагов.
- **Что на входе**: Схема `train_sdf`; список явных категориальных `preprocessing.categorical_encoding.force_categorical`; служебные поля `index_col`, `target_col`, `skip_cols`.
- **Результат**: Заполнены `cat_cols`, `num_cols`, `final_features_list`. Неизвестные Spark-типы сообщаются в логах как предупреждение и не включаются в обработку.

### rebalancing – ребалансировка таргета
- **Назначение**: Добавление баланса классов в `train_sdf` путём даунсемплинга отрицательного класса до заданной доли положительного.
- **Что на входе**: `preprocessing.rebalancing.positive_frac`, бинарный `target_col` в `train_sdf`.
- **Результат**: Обновлённый `train_sdf` с новым соотношением классов, отчёт о распределении классов и количестве строк после ребалансировки.

### drop_forced_cols – удаление фичей по списку
- **Назначение**: Жёсткое исключение заведомо нежелательных признаков из всех датасетов (например, утечки, служебные поля).
- **Что на входе**: Список `common.force_drop_cols`. Датафреймы `train_sdf`/`valid_sdf`/`scoring_sdf`.
- **Результат**: Колонки удалены во всех датасетах, список зафиксирован в `artifacts/drop_forced_cols.txt`, актуализированы `cat_cols`/`num_cols`/`final_features_list` и их файлы.

### convert_decimal – decimal → float
- **Назначение**: Приведение колонок `DecimalType` к `FloatType` для совместимости с pandas/sklearn и унификации числовой обработки.
- **Что на входе**: Схемы датафреймов; список `skip_cols` для игнорирования.
- **Результат**: В числовых колонках типа `DecimalType` меняется тип на `FloatType` во всех датасетах.

### fill_missing – заполнение пропусков
- **Назначение**: Единообразное заполнение пропусков по типам признаков для стабилизации последующих шагов и моделей.
- **Что на входе**: Значения `preprocessing.missing_values.numeric` и `preprocessing.missing_values.categorical`; списки `cat_cols` и `num_cols`.
- **Результат**: Заполненные пропуски в соответствующих колонках всех датасетов.

### drop_uninformative – удаление малоинформативных фичей
- **Назначение**: Исключение признаков с доминирующим модальным значением, не несущих различающей информации.
- **Что на входе**: Порог `preprocessing.drop_uninformative.threshold`; набор признаков из `train_sdf`, исключая `index_col`, `target_col`, `skip_cols`.
- **Результат**: Удалённые колонки с долей модального значения ≥ порога; сохранён список в `artifacts/drop_uninformative_cols.txt`; обновлены рабочие списки и файлы со списками признаков.

### deduplicate – удаление дубликатов
- **Назначение**: Устранение дублирующихся записей для корректных метрик и обучения.
- **Что на входе**: Подмножество ключевых колонок `preprocessing.deduplicate.subset` (по умолчанию `[index_col]`); датафреймы `train_sdf` и, при наличии, `valid_sdf`.
- **Результат**: При наличии valid — объединение train+valid, удаление дублей, обратное разделение; отчёт о числе удалённых строк. При отсутствии valid — дедупликация только train.

## Преобразования признаков

### categorical_encoding – кодирование категориальных фичей
- **Назначение**: Преобразование строковых признаков в числовые представления для последующей работы моделей и скейлеров.
- **Что на входе**: Метод `preprocessing.categorical_encoding.method` (`label_encoder`/`target_encoder`); `cat_cols`; опционально `preprocessing.categorical_encoding.sample_size` для ускорения фита; `target_col` при target encoding.
- **Результат**: Обученный энкодер (`encoder.pkl`) и преобразованные датасеты через `mapInPandas`. При `target_encoder` соответствующие колонки переводятся в числовые и переносятся из `cat_cols` в `num_cols`; списки и их файлы актуализируются.

### scaling – масштабирование фичей
- **Назначение**: Приведение числовых признаков к сопоставимым шкалам для устойчивого обучения.
- **Что на входе**: Метод `preprocessing.scaling.method` (`standard`/`minmax`/`quantile`/`binning`), опционально `preprocessing.scaling.sample_size` и `preprocessing.scaling.bins` (для биннинга); список `num_cols`.
- **Результат**: Обученный скейлер (`scaler.pkl`) и преобразованные датасеты через `mapInPandas`; числовые колонки приводятся к `FloatType` в схемах.

## Отбор признаков (Feature selectors)

### drop_correlated – удаление коррелирующих фичей (опц.)
- **Назначение**: Снижение мультиколлинеарности и избыточности признаков, улучшение обобщающей способности моделей.
- **Что на входе**: Конфигурация селектора `CorrFeatureRemover` (порог корреляции, способ оценки важности по фолдам и др. из соответствующей секции конфига); текущий `train_sdf`.
- **Результат**: Список исключённых признаков с высокой взаимной корреляцией; сохранение в `artifacts/drop_correlated_cols.txt`; обновление рабочих списков и их файлов.

### drop_adversarial – удаление adversarial фичей (опц.)
- **Назначение**: Исключение признаков, позволяющих модели отличить train от valid (shift/утечки), чтобы повысить переносимость и стабильность скоринга.
- **Что на входе**: Конфигурация `AdversarialFeatureRemover`; датафреймы `train_sdf` и (опционально) `valid_sdf`; списки `final_features_list` и `cat_cols`. `scoring_sdf` намеренно не участвует.
- **Результат**: Сформированный список исключённых колонок с сохранением в `artifacts/drop_adversarial_cols.txt`; обновлённые списки признаков и их файловые представления.

### Forward Selection
- **Назначение**: Поэтапное включение признаков по убыванию их полезности с приёмом только тех, что улучшают метрику/проходят голосование фолдов.
- **Что на входе**: Конфигурация `ForwardFeatureSelector` (метрика, количество фолдов, стратегия принятия); `train_sdf`, рабочие списки признаков.
- **Результат**: Итоговый набор принятых признаков; непринятые исключаются и сохраняются в `artifacts/fs_excluded_cols.txt`; обновляются списки и их файлы.

### Noise Selection
- **Назначение**: Отсев слабых признаков относительно синтетического шума, чтобы оставить только устойчиво полезные по фолдам.
- **Что на входе**: Конфигурация `NoiseFeatureSelector` (как формируется шум, правила порогов и голосования); `train_sdf`, списки признаков и категориальные индикаторы.
- **Результат**: Исключённые признаки фиксируются в `artifacts/noise_excluded_cols.txt`; списки и их файлы актуализируются.

### Permutation Selection
- **Назначение**: Оценка значимости признаков через permutation importance по фолдам и отбор по порогам и стабильности.
- **Что на входе**: Конфигурация `PermutationSelector` (метрика, пороги, число повторов/фолдов); `train_sdf`, рабочие списки признаков.
- **Результат**: Исключённые признаки сохраняются в `artifacts/permutation_excluded_cols.txt`; списки признаков обновляются и записываются в соответствующие файлы.

## Артефакты и результаты

### save_artifacts_info – сохранение артефактов
- **Назначение**: Централизованная фиксация путей ко всем сохранённым артефактам для воспроизводимости и последующего применения.
- **Что на входе**: Сформированная структура `artifacts_info` внутри пайплайна, включающая пути к спискам удалённых колонок и файлам трансформеров.
- **Результат**: JSON `artifacts_info.json` в каталоге `artifacts.path` с ключами: `drop_forced_cols_path`, `drop_uninformative_cols_path`, `drop_correlated_cols_path`, `drop_adversarial_cols_path`, `noise_excluded_cols_path`, `fs_excluded_cols_path`, `permutation_excluded_cols_path`, `encoder_path`, `scaler_path`.

### encoder.pkl / scaler.pkl
- **Назначение**: Сериализованные обученные трансформеры для повторного применения на новых данных и в проде.
- **Что на входе**: Обученный на train-сэмпле энкодер/скейлер.
- **Результат**: Файлы `encoder.pkl` и `scaler.pkl` в каталоге артефактов; пути также отражаются в `artifacts_info.json`.

### final_feature_list.txt / final_categorical_list.txt
- **Назначение**: Постоянное отражение актуального состава признаков после каждого шага удаления/отбора.
- **Что на входе**: Текущие списки `final_features_list` и `cat_cols` внутри пайплайна.
- **Результат**: Файлы `final_feature_list.txt` и `final_categorical_list.txt` в каталоге артефактов; лог с количеством общих и категориальных признаков.

### save_results – сохранение выборок
- **Назначение**: Экспорт обработанных выборок в согласованном формате для downstream-задач (обучение моделей, валидация, инференс).
- **Что на входе**: Обновлённые `train_sdf`/`valid_sdf`/`scoring_sdf` (по мере наличия); секция `result` конфига: `format` (`parquet`/`hive`), `save_path`, `suffix`, а также `format_scoring` и `save_path_scoring` для скоринга.
- **Результат**: Сохранённые датасеты: для Parquet — файлы `train{suffix}.parquet`, `test{suffix}.parquet`, `scoring{suffix}.parquet` в соответствующих директориях; для Hive — одноимённые таблицы, перезаписываемые через `saveAsTable` с предварительным `DROP TABLE IF EXISTS`.


